{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be5a4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b81e695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simple regression problem\n",
    "TRAINING_POINTS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1de497f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "# With this block, we don't need to set device=DEVICE for every tensor.\n",
    "# But you will still need to avoid accidentally getting int types instead of floating-point types.\n",
    "torch.set_default_dtype(torch.float32)\n",
    "if torch.cuda.is_available():\n",
    "     torch.set_default_device(0)\n",
    "     print(\"Running on the GPU\")\n",
    "else:\n",
    "     print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b912d6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_linear_training_data():\n",
    "    \"\"\"\n",
    "    This method simply rotates points in a 2D space.\n",
    "    Be sure to use MSE in the place of the final softmax layer before testing on this\n",
    "    data!\n",
    "    :return: (x,y) the dataset. x is a torch tensor where columns are training samples and\n",
    "             y is a torch tensor where each column is an output sample, rotated 90 degrees \n",
    "             in 2d space from the input point. \n",
    "    \"\"\"\n",
    "    x = torch.randn((2, TRAINING_POINTS))\n",
    "    x1 = x[0:1, :].clone()\n",
    "    x2 = x[1:2, :]\n",
    "    y = torch.cat((-x2, x1), axis=0)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae62f59a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_folded_training_data():\n",
    "    \"\"\"\n",
    "    This method introduces a single non-linear fold into the sort of data created by create_linear_training_data.\n",
    "    Be sure to use MSE in the place of the final softmax layer before testing on this\n",
    "    data!\n",
    "    :return: (x,y) the dataset. x is a torch tensor where columns are training samples and\n",
    "             y is a torch tensor where columns output samples. \n",
    "    \"\"\"\n",
    "    x = torch.randn((2, TRAINING_POINTS))\n",
    "    x1 = x[0:1, :].clone()\n",
    "    x2 = x[1:2, :]\n",
    "    x2 *= 2 * ((x2 > 0).float() - 0.5)\n",
    "    y = torch.cat((-x2, x1), axis=0)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fa7df6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_square():\n",
    "    \"\"\"\n",
    "    This is a square example in which the challenge is to determine\n",
    "    if the points are inside or outside of a square in 2d space.\n",
    "    insideness is true if the points are inside the square.\n",
    "    :return: (points, insideness) the dataset. points is a 2xN array of points and insideness is true if the point is inside the square.\n",
    "    \"\"\"\n",
    "    win_x = [2,2,3,3]\n",
    "    win_y = [1,2,2,1]\n",
    "    win = torch.tensor([win_x,win_y],dtype=torch.float32)\n",
    "    win_rot = torch.cat((win[:,1:],win[:,0:1]),axis=1)\n",
    "    t = win_rot - win # edges tangent along side of poly\n",
    "    rotation = torch.tensor([[0, 1],[-1,0]],dtype=torch.float32)\n",
    "    normal = rotation @ t # normal vectors to each side of poly\n",
    "        # torch.matmul(rotation,t) # Same thing\n",
    "\n",
    "    points = torch.rand((2,2000),dtype = torch.float32)\n",
    "    points = 4*points\n",
    "\n",
    "    vectors = points[:,np.newaxis,:] - win[:,:,np.newaxis] # reshape to fill origin\n",
    "    insideness = (normal[:,:,np.newaxis] * vectors).sum(axis=0)\n",
    "    insideness = insideness.T\n",
    "    insideness = insideness > 0\n",
    "    insideness = insideness.all(axis=1)\n",
    "    return points, insideness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2626370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # For this week's lab, you can likely keep ALL your code\n",
    "    # right here, with all your variables in the global scope \n",
    "    # for simplified debugging.\n",
    "    #\n",
    "    # The code in this section should NOT be in a helper method.\n",
    "    # Similarly, the output of each sub-layer and activation function should\n",
    "    # be in this scope.\n",
    "\n",
    "    # TODO: You may wish to make each TODO below its own pynb cell.\n",
    "    # TODO: Build your network.\n",
    "\n",
    "    # TODO: Select your datasource.\n",
    "    x_train, y_train = create_linear_training_data()\n",
    "\n",
    "    # TODO: Train your network.\n",
    "\n",
    "    # TODO: Sanity-check the output of your network.\n",
    "    # You can optionally compute the error on this test data:\n",
    "    x_test, y_test = create_linear_training_data()\n",
    "\n",
    "    # But you must computed W*M as discussed in the lab assignment.\n",
    "\n",
    "    pass # You may wish to keep this line as a point to place a debugging breakpoint."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
