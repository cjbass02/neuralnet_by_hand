{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9be5a4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b81e695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simple regression problem\n",
    "TRAINING_POINTS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1de497f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the CPU\n"
     ]
    }
   ],
   "source": [
    "# With this block, we don't need to set device=DEVICE for every tensor.\n",
    "# But you will still need to avoid accidentally getting int types instead of floating-point types.\n",
    "torch.set_default_dtype(torch.float32)\n",
    "if torch.cuda.is_available():\n",
    "     torch.set_default_device(0)\n",
    "     print(\"Running on the GPU\")\n",
    "else:\n",
    "     print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29b912d6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_linear_training_data():\n",
    "    \"\"\"\n",
    "    This method simply rotates points in a 2D space.\n",
    "    Be sure to use MSE in the place of the final softmax layer before testing on this\n",
    "    data!\n",
    "    :return: (x,y) the dataset. x is a torch tensor where columns are training samples and\n",
    "             y is a torch tensor where each column is an output sample, rotated 90 degrees \n",
    "             in 2d space from the input point. \n",
    "    \"\"\"\n",
    "    x = torch.randn((2, TRAINING_POINTS))\n",
    "    x1 = x[0:1, :].clone()\n",
    "    x2 = x[1:2, :]\n",
    "    y = torch.cat((-x2, x1), axis=0)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae62f59a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_folded_training_data():\n",
    "    \"\"\"\n",
    "    This method introduces a single non-linear fold into the sort of data created by create_linear_training_data.\n",
    "    Be sure to use MSE in the place of the final softmax layer before testing on this\n",
    "    data!\n",
    "    :return: (x,y) the dataset. x is a torch tensor where columns are training samples and\n",
    "             y is a torch tensor where columns output samples. \n",
    "    \"\"\"\n",
    "    x = torch.randn((2, TRAINING_POINTS))\n",
    "    x1 = x[0:1, :].clone()\n",
    "    x2 = x[1:2, :]\n",
    "    x2 *= 2 * ((x2 > 0).float() - 0.5)\n",
    "    y = torch.cat((-x2, x1), axis=0)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4fa7df6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_square():\n",
    "    \"\"\"\n",
    "    This is a square example in which the challenge is to determine\n",
    "    if the points are inside or outside of a square in 2d space.\n",
    "    insideness is true if the points are inside the square.\n",
    "    :return: (points, insideness) the dataset. points is a 2xN array of points and insideness is true if the point is inside the square.\n",
    "    \"\"\"\n",
    "    win_x = [2,2,3,3]\n",
    "    win_y = [1,2,2,1]\n",
    "    win = torch.tensor([win_x,win_y],dtype=torch.float32)\n",
    "    win_rot = torch.cat((win[:,1:],win[:,0:1]),axis=1)\n",
    "    t = win_rot - win # edges tangent along side of poly\n",
    "    rotation = torch.tensor([[0, 1],[-1,0]],dtype=torch.float32)\n",
    "    normal = rotation @ t # normal vectors to each side of poly\n",
    "        # torch.matmul(rotation,t) # Same thing\n",
    "\n",
    "    points = torch.rand((2,2000),dtype = torch.float32)\n",
    "    points = 4*points\n",
    "\n",
    "    vectors = points[:,np.newaxis,:] - win[:,:,np.newaxis] # reshape to fill origin\n",
    "    insideness = (normal[:,:,np.newaxis] * vectors).sum(axis=0)\n",
    "    insideness = insideness.T\n",
    "    insideness = insideness > 0\n",
    "    insideness = insideness.all(axis=1)\n",
    "    return points, insideness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4682a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return torch.maximum(x, torch.tensor(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2626370f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4685, -0.1644,  0.3091,  ...,  0.0128,  0.0573, -1.5272],\n",
      "        [ 0.9017, -1.6444, -0.7179,  ...,  0.0781,  0.8734,  0.6782]])\n",
      "tensor([[-0.9017,  1.6444,  0.7179,  ..., -0.0781, -0.8734, -0.6782],\n",
      "        [-0.4685, -0.1644,  0.3091,  ...,  0.0128,  0.0573, -1.5272]])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # For this week's lab, you can likely keep ALL your code\n",
    "    # right here, with all your variables in the global scope \n",
    "    # for simplified debugging.\n",
    "    #\n",
    "    # The code in this section should NOT be in a helper method.\n",
    "    # Similarly, the output of each sub-layer and activation function should\n",
    "    # be in this scope.\n",
    "\n",
    "    # TODO: You may wish to make each TODO below its own pynb cell.\n",
    "    # TODO: Build your network.\n",
    "\n",
    "    # TODO: Select your datasource.\n",
    "    x_train, y_train = create_linear_training_data()\n",
    "    print(x_train)\n",
    "    print(y_train)\n",
    "    # TODO: Train your network.\n",
    "\n",
    "    # TODO: Sanity-check the output of your network.\n",
    "    # You can optionally compute the error on this test data:\n",
    "    x_test, y_test = create_linear_training_data()\n",
    "\n",
    "    # But you must computed W*M as discussed in the lab assignment.\n",
    "\n",
    "    pass # You may wish to keep this line as a point to place a debugging breakpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955b58df",
   "metadata": {},
   "source": [
    "### Test 1\n",
    "Output should be [0.2185, 0.027], J should be 0.18101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6c126d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[0.105, 0.815]])  \n",
    "W1 = torch.tensor([[0.75, -0.55, 0.25], [0.25, -0.75, -0.15]], requires_grad=True)\n",
    "b1 = torch.tensor([-0.1, 0.2, -0.3], requires_grad=True)  \n",
    "W2 = torch.tensor([[0.1, -0.4], [-0.2, 0.5], [0.3, -0.6]], requires_grad=True)  \n",
    "b2 = torch.tensor([0.2, 0.1], requires_grad=True)  \n",
    "Y_true = torch.tensor([[0.815, 0.105]])\n",
    "lambda_l2 = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "025f6bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output (Y_pred): tensor([[0.2183, 0.0270]], grad_fn=<AddBackward0>)\n",
      "True Output (Y_true): tensor([[0.8150, 0.1050]])\n",
      "MSE Loss: 0.18109728395938873\n",
      "L2 Regularization Loss: 6.500001472886652e-05\n",
      "Total Loss (MSE + L2 Regularization): 0.1811622828245163\n"
     ]
    }
   ],
   "source": [
    "Z1 = X @ W1 + b1\n",
    "A1 = relu(Z1) \n",
    "Z2 = A1 @ W2 + b2\n",
    "Y_pred = Z2  \n",
    "mse_loss = torch.mean((Y_pred - Y_true) ** 2)\n",
    "l2_reg = lambda_l2/2 * (torch.sum(W1)**2 + torch.sum(W2)**2)\n",
    "total_loss = mse_loss + l2_reg\n",
    "\n",
    "print(\"Predicted Output (Y_pred):\", Y_pred)\n",
    "print(\"True Output (Y_true):\", Y_true)\n",
    "print(\"MSE Loss:\", mse_loss.item())\n",
    "print(\"L2 Regularization Loss:\", l2_reg.item())\n",
    "print(\"Total Loss (MSE + L2 Regularization):\", total_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c70366",
   "metadata": {},
   "source": [
    "These values are very close to the output and loss I calculated by hand. The small differences are due to the rounding I did by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19918b99",
   "metadata": {},
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f9dbb",
   "metadata": {},
   "source": [
    "Output should be [-2.0, 0.5]. J should be 2.215."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "949d104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[1.0, 0.0]])  \n",
    "W1 = torch.tensor([[1.0, 0.0, 1.0], [0.0, 1.0, 0.0]], requires_grad=True)\n",
    "b1 = torch.tensor([.5, .5, .5], requires_grad=True)  \n",
    "W2 = torch.tensor([[-1.0, 0.0], [0.0, -1.0], [-1.0, 0.0]], requires_grad=True)  \n",
    "b2 = torch.tensor([1.0, 1.0], requires_grad=True)  \n",
    "Y_true = torch.tensor([[0.0, 1.0]])\n",
    "lambda_l2 = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7d0076e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output (Y_pred): tensor([[-2.0000,  0.5000]], grad_fn=<AddBackward0>)\n",
      "True Output (Y_true): tensor([[0., 1.]])\n",
      "MSE Loss: 2.125\n",
      "L2 Regularization Loss: 0.08999999612569809\n",
      "Total Loss (MSE + L2 Regularization): 2.2149999141693115\n"
     ]
    }
   ],
   "source": [
    "Z1 = X @ W1 + b1\n",
    "A1 = relu(Z1) \n",
    "Z2 = A1 @ W2 + b2\n",
    "Y_pred = Z2  \n",
    "mse_loss = torch.mean((Y_pred - Y_true) ** 2)\n",
    "l2_reg = lambda_l2/2 * (torch.sum(W1)**2 + torch.sum(W2)**2)\n",
    "total_loss = mse_loss + l2_reg\n",
    "\n",
    "print(\"Predicted Output (Y_pred):\", Y_pred)\n",
    "print(\"True Output (Y_true):\", Y_true)\n",
    "print(\"MSE Loss:\", mse_loss.item())\n",
    "print(\"L2 Regularization Loss:\", l2_reg.item())\n",
    "print(\"Total Loss (MSE + L2 Regularization):\", total_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06851ab6",
   "metadata": {},
   "source": [
    "These values are very close to the output and loss I calculated by hand. The small differences are due to the rounding I did by hand."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
